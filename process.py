# -*- coding: utf-8 -*-
"""製程資料分析.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZeaQi6bd5d5xxQEDXgQ_bKq4-2gH8mEj

##使用特徵值
"""

# prompt: 幫我保留"Lot_RI"的欄位

import chardet
import pandas as pd

# ##使用特徵值

# 檢測檔案的編碼
with open('專案資料3.xlsx', 'rb') as file:
    result = chardet.detect(file.read(10000))
    print(result['encoding'])
data = pd.read_excel('專案資料3.xlsx')
# 計算每個欄位缺失值的比例
missing_ratio = data.isnull().mean()

# 篩選缺失值比例小於 80% 的欄位
columns_to_keep = missing_ratio[missing_ratio < 0.8].index
data = data[columns_to_keep]

# 選擇文字型態的欄位
text_columns = data.select_dtypes(include=['object']).columns

# 列出需要保留的欄位，確保'Lot_RI'在其中
columns_to_keep = [
    'Judgment_QA', 'Shift_IPQC', 'AppearanceDescription',
    'RRP_R03_CheckResult', 'RRP_R11_CheckResult', 'RSS_002_CheckResult',
    'RSS_011_CheckResult', 'RSS_015_CheckResult','Lot_RI', # 保留 Lot_RI
    # 新增的保留欄位
    'RRP_R03_Field7', 'RRP_R11_Field7', 'RSS_002_Field3', 'RSS_011_Field1',
    'RSS_011_Field3', 'RSS_011_Field5', 'RSS_015_Field3',
    'RES_013_CheckResult_Top', 'RFF_002_CheckResult_Top',
    'RPA_507_CheckResult_Top', 'RPA_520_CheckResult_Top',
    'RSS_006_CheckResult_Top', 'RSS_011_CheckResult_Top',
    'RSS_025_CheckResult_Top', 'RES_013_Field7_Top', 'RPA_507_Field7_Top',
    'RPA_520_Field7_Top', 'RSS_006_Field3_Top', 'RSS_011_Field1_Top',
    'RSS_011_Field3_Top', 'RSS_011_Field5_Top', 'RSS_025_Field3_Top',
    'RAK_129_CheckResult_AD', 'RAP_128_CheckResult_AD',
    'RFF_F04_CheckResult_AD', 'RSS_015_CheckResult_AD',
    'RAK_129_Field7_AD', 'RAP_128_Field7_AD', 'RSS_015_Field3_AD'
]

# 過濾出需要刪除的文字型態欄位
columns_to_delete = [col for col in text_columns if col not in columns_to_keep]

# 列出刪除的欄位
print("刪除以下文字型態的欄位:")
for col in columns_to_delete:
    print(col)

# 刪除文字型態欄位（但保留需要保留的欄位，包含'Lot_RI'）
data = data.drop(columns=columns_to_delete, errors='ignore') # 使用errors='ignore'避免因欄位不存在而報錯

# 顯示處理後的資料
print("處理後的資料：")
print(data.head())

from sklearn.preprocessing import StandardScaler, LabelEncoder

# 分別選取數值型態與文字型態的欄位
numeric_columns = data.select_dtypes(include=['number']).columns
text_columns = data.select_dtypes(include=['object']).columns
'''
# 初始化標準化器
scaler = StandardScaler()

# 對數值欄位進行標準化
data = data.copy()
data[numeric_columns] = data[numeric_columns].apply(
    lambda col: col.fillna(col.median())  # 填補缺失值為中位數
)
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])
'''
# 初始化 LabelEncoder
label_encoders = {}
for col in text_columns:
    # 填補文字型態欄位的缺失值為眾數
    most_frequent_value = data[col].mode()[0]  # 計算眾數
    data[col] = data[col].fillna(most_frequent_value)

    # 對欄位進行 LabelEncoding
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col].astype(str))  # 將文字轉為數字
    label_encoders[col] = le  # 保存每個欄位的 LabelEncoder 以供後續使用

# 顯示標準化與 LabelEncoding 後的資料
print("標準化與 LabelEncoding 完成！")
print(data.head())

# 選擇日期型態的欄位
date_columns = data.select_dtypes(include=['datetime']).columns

# 刪除日期型態欄位
data = data.drop(columns=date_columns)

# 列出被刪除的日期欄位
print("刪除以下日期型態的欄位:")
print(date_columns)

columns_to_delete = ['CheckBasID_QA','CheckSeq_QA','QABasID_QA','PutMeters_QA','AMeters_QA','A1Meters_QA','GiveMeters_QA','BFaultMeters_QA','CFaultMeters_QA','MRBMeters_QA',
'OverMeters_QA','ProductCode_QA','AOD_QA','Returned_QA','QtyReceived_QA','ProductCodeSNo_QA', 'PoLine_QA','QAChecked_QA','QAResultErrorOfUnit_QA','KilogramToDrum_QA','QtyReceivedOfUnit_QA',
'QAResultErrorOfUnit_QA','IPQCBasID_IPQC','PhyBasID_IPQC','DtlID_IPQC','A72_RI','A155_AL','A72_AD','A73_AD','B73_RI','DiffWithMaxA73_AD','DiffValue',
'VC3_RTO_FAN1_PM_RI','NaturalGas_AD','B7_RI','A73_RI']
data = data.drop(columns=columns_to_delete)

data = data.rename(columns={
    "B版切面": "BSectionScoreEnd_IPQC",
    "剝離力": "PeelForce_IPQC",
    "破膜力": "RupturePeelForce_IPQC",
    "乾量": "DryBasis_IPQC",
})

data

data = data.groupby("Lot_RI").mean().reset_index()
data

from sklearn.preprocessing import StandardScaler, LabelEncoder

# 分別選取數值型態與文字型態的欄位
numeric_columns = data.select_dtypes(include=['number']).columns
numeric_columns = numeric_columns.drop("Judgment_QA")  # 排除 Judgment_QA

# 初始化標準化器
scaler = StandardScaler()

# 對數值欄位進行標準化
data = data.copy()
data[numeric_columns] = data[numeric_columns].apply(
    lambda col: col.fillna(col.median())  # 填補缺失值為中位數
)
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])
print(data.head())

pip install catboost

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report
from imblearn.over_sampling import SMOTE

# 修改部分：加入混淆矩陣記錄
model_confusion_matrices = {model_name: [] for model_name in models.keys()}  # 每個模型的混淆矩陣

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 10 特徵進行重新訓練和驗證")
    top_features = selected_features_per_model[model_name]

    # 僅使用前 10 個特徵
    X_selected = X[top_features]

    # 交叉驗證過程
    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 使用 SMOTE 對訓練集進行過採樣
        smote = SMOTE()
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)

        # 訓練模型
        model.fit(X_train_resampled, y_train_resampled)

        # 計算訓練集與測試集準確率
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        test_accuracy = accuracy_score(y_val_fold, y_val_pred)

        model_accuracies[model_name]["train"].append(train_accuracy)
        model_accuracies[model_name]["test"].append(test_accuracy)

        # **計算混淆矩陣**
        cm = confusion_matrix(y_val_fold, y_val_pred)
        model_confusion_matrices[model_name].append(cm)

    # 計算平均混淆矩陣
    total_cm = np.sum(model_confusion_matrices[model_name], axis=0)
    print(f"{model_name} 的累計混淆矩陣：\n{total_cm}")

    # 繪製混淆矩陣
    disp = ConfusionMatrixDisplay(confusion_matrix=total_cm, display_labels=model.classes_)
    disp.plot(cmap="Blues")
    plt.title(f"{model_name} 的混淆矩陣")
    plt.show()

    # 平均準確率輸出
    train_acc_mean = np.mean(model_accuracies[model_name]["train"])
    test_acc_mean = np.mean(model_accuracies[model_name]["test"])
    print(f"{model_name} 平均訓練準確率：{train_acc_mean:.4f}")
    print(f"{model_name} 平均測試準確率：{test_acc_mean:.4f}")

    # **分類報告**
    print(f"{model_name} 的分類報告：")
    print(classification_report(y_val_fold, y_val_pred))

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report
from imblearn.over_sampling import SMOTE
import numpy as np
import pandas as pd

# 初始化模型的混淆矩陣和評估結果存儲
model_confusion_matrices = {model_name: [] for model_name in models.keys()}
model_reports = {model_name: [] for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 10 特徵進行重新訓練和驗證")
    top_features = selected_features_per_model[model_name]

    # 僅使用前 10 個特徵
    X_selected = X[top_features]

    # 初始化交叉驗證存儲
    train_accuracies, test_accuracies = [], []

    # 交叉驗證過程
    for train_idx, val_idx in kfold.split(X_selected, y):
        # 分割數據
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 使用 SMOTE 過採樣
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)

        # 訓練模型
        model.fit(X_train_resampled, y_train_resampled)

        # 訓練集和測試集預測
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        # 計算準確率
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        test_accuracy = accuracy_score(y_val_fold, y_val_pred)

        train_accuracies.append(train_accuracy)
        test_accuracies.append(test_accuracy)

        # 混淆矩陣計算並存儲
        cm = confusion_matrix(y_val_fold, y_val_pred)
        model_confusion_matrices[model_name].append(cm)

        # 分類報告
        report = classification_report(y_val_fold, y_val_pred, output_dict=True)
        model_reports[model_name].append(report)

    # 累計混淆矩陣
    total_cm = np.sum(model_confusion_matrices[model_name], axis=0)
    print(f"{model_name} 的累計混淆矩陣：\n{total_cm}")

    # 繪製混淆矩陣
    disp = ConfusionMatrixDisplay(confusion_matrix=total_cm, display_labels=model.classes_)
    disp.plot(cmap="Blues")
    plt.title(f"{model_name} 的混淆矩陣")
    plt.show()

    # 平均準確率輸出
    train_acc_mean = np.mean(train_accuracies)
    test_acc_mean = np.mean(test_accuracies)
    print(f"{model_name} 平均訓練準確率：{train_acc_mean:.4f}")
    print(f"{model_name} 平均測試準確率：{test_acc_mean:.4f}")

    # 平均分類報告計算
    avg_report = {}
    for key in model_reports[model_name][0].keys():
        avg_report[key] = {}
        if isinstance(model_reports[model_name][0][key], dict):  # 如果是嵌套字典
            for metric in model_reports[model_name][0][key].keys():
                avg_report[key][metric] = np.mean(
                    [report[key][metric] for report in model_reports[model_name] if key in report]
                )
        else:  # 如果是單一值
            avg_report[key] = np.mean(
                [report[key] for report in model_reports[model_name] if key in report]
            )

    print(f"{model_name} 的平均分類報告：")
    for label, metrics in avg_report.items():
        if isinstance(metrics, dict):
            print(f"  類別 {label}: 精確率 {metrics['precision']:.2f}, 召回率 {metrics['recall']:.2f}, F1分數 {metrics['f1-score']:.2f}")
        else:
            print(f"  {label}: {metrics:.2f}")





"""##原始資料"""

import chardet

# 檢測檔案的編碼
with open('專案資料3.xlsx', 'rb') as file:
    result = chardet.detect(file.read(10000))
    print(result['encoding'])

import pandas as pd
data = pd.read_excel('專案資料3.xlsx')

# 將第 0 列設為標題
#data.columns = data.iloc[0]

# 刪除第 0 列，因為它已經被用作標題
#data = data[1:]

# 重置索引
#data.reset_index(drop=True, inplace=True)

data

data.columns



# 計算每個欄位缺失值的比例
missing_ratio = data.isnull().mean()

# 篩選缺失值比例小於 80% 的欄位
columns_to_keep = missing_ratio[missing_ratio < 0.8].index
data = data[columns_to_keep]
data

# 選擇文字型態的欄位
text_columns = data.select_dtypes(include=['object']).columns

# 列出需要保留的欄位
columns_to_keep = [
    'Judgment_QA', 'Shift_IPQC', 'AppearanceDescription',
    'RRP_R03_CheckResult', 'RRP_R11_CheckResult', 'RSS_002_CheckResult',
    'RSS_011_CheckResult', 'RSS_015_CheckResult',
    # 新增的保留欄位
    'RRP_R03_Field7', 'RRP_R11_Field7', 'RSS_002_Field3', 'RSS_011_Field1',
    'RSS_011_Field3', 'RSS_011_Field5', 'RSS_015_Field3',
    'RES_013_CheckResult_Top', 'RFF_002_CheckResult_Top',
    'RPA_507_CheckResult_Top', 'RPA_520_CheckResult_Top',
    'RSS_006_CheckResult_Top', 'RSS_011_CheckResult_Top',
    'RSS_025_CheckResult_Top', 'RES_013_Field7_Top', 'RPA_507_Field7_Top',
    'RPA_520_Field7_Top', 'RSS_006_Field3_Top', 'RSS_011_Field1_Top',
    'RSS_011_Field3_Top', 'RSS_011_Field5_Top', 'RSS_025_Field3_Top',
    'RAK_129_CheckResult_AD', 'RAP_128_CheckResult_AD',
    'RFF_F04_CheckResult_AD', 'RSS_015_CheckResult_AD',
    'RAK_129_Field7_AD', 'RAP_128_Field7_AD', 'RSS_015_Field3_AD'
]

# 過濾出需要刪除的文字型態欄位
columns_to_delete = [col for col in text_columns if col not in columns_to_keep]

# 列出刪除的欄位
print("刪除以下文字型態的欄位:")
for col in columns_to_delete:
    print(col)

# 刪除文字型態欄位（但保留需要保留的欄位）
data = data.drop(columns=columns_to_delete)

# 顯示處理後的資料
print("處理後的資料：")
print(data.head())

data

from sklearn.preprocessing import StandardScaler, LabelEncoder

# 分別選取數值型態與文字型態的欄位
numeric_columns = data.select_dtypes(include=['number']).columns
text_columns = data.select_dtypes(include=['object']).columns

# 初始化標準化器
scaler = StandardScaler()

# 對數值欄位進行標準化
data = data.copy()
data[numeric_columns] = data[numeric_columns].apply(
    lambda col: col.fillna(col.median())  # 填補缺失值為中位數
)
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])

# 初始化 LabelEncoder
label_encoders = {}
for col in text_columns:
    # 填補文字型態欄位的缺失值為眾數
    most_frequent_value = data[col].mode()[0]  # 計算眾數
    data[col] = data[col].fillna(most_frequent_value)

    # 對欄位進行 LabelEncoding
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col].astype(str))  # 將文字轉為數字
    label_encoders[col] = le  # 保存每個欄位的 LabelEncoder 以供後續使用

# 顯示標準化與 LabelEncoding 後的資料
print("標準化與 LabelEncoding 完成！")
print(data.head())

# 選擇日期型態的欄位
date_columns = data.select_dtypes(include=['datetime']).columns

# 刪除日期型態欄位
data = data.drop(columns=date_columns)

# 列出被刪除的日期欄位
print("刪除以下日期型態的欄位:")
print(date_columns)

# 列出所有欄位名稱
print("資料的所有欄位名稱:")
for col in data.columns:
    print(col)

columns_to_delete = ['CheckBasID_QA','CheckSeq_QA','QABasID_QA','PutMeters_QA','AMeters_QA','A1Meters_QA','GiveMeters_QA','BFaultMeters_QA','CFaultMeters_QA','MRBMeters_QA',
'OverMeters_QA','ProductCode_QA','AOD_QA','Returned_QA','QtyReceived_QA','ProductCodeSNo_QA', 'PoLine_QA','QAChecked_QA','QAResultErrorOfUnit_QA','KilogramToDrum_QA','QtyReceivedOfUnit_QA',
'QAResultErrorOfUnit_QA','IPQCBasID_IPQC','PhyBasID_IPQC','DtlID_IPQC','A72_RI','A155_AL','A72_AD','A73_AD','B73_RI','DiffWithMaxA73_AD','DiffValue',
'VC3_RTO_FAN1_PM_RI','NaturalGas_AD','B7_RI','A73_RI','NaturalGas_RI']
data = data.drop(columns=columns_to_delete)

data

data = data.rename(columns={
    "B版切面": "BSectionScoreEnd_IPQC",
    "剝離力": "PeelForce_IPQC",
    "破膜力": "RupturePeelForce_IPQC",
    "乾量": "DryBasis_IPQC",
})

pip install catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from catboost import CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# 假设 data 已经标准化，并且处理了缺失值
X = data.drop(columns=['Judgment_QA'])  # 独立变量
y = data['Judgment_QA']  # 目标变量

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "CatBoost": CatBoostClassifier(verbose=0)
}

# 初始化交叉验证器
kfold = StratifiedKFold(n_splits=5, shuffle=True)

# **1. 独立挑选每个模型的前 10 特征**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑选 {model_name} 的前 10 个重要特征")
    feature_importances = []

    # 使用交叉验证计算特征重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特征重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 创建特征重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 选择前 10 个重要特征
    top_features = importance_df.head(10)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 10 特征：\n{top_features}")

# **2. 使用前 10 特征进行交叉验证训练**
final_feature_importance_results = {}
model_accuracies = {model_name: {"train": [], "test": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 10 特征进行重新训练和验证")
    top_features = selected_features_per_model[model_name]

    # 仅使用前 10 个特征
    X_selected = X[top_features]

    # 记录特征重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 记录训练集和测试集准确率
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        test_accuracy = accuracy_score(y_val_fold, y_val_pred)

        model_accuracies[model_name]["train"].append(train_accuracy)
        model_accuracies[model_name]["test"].append(test_accuracy)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 计算平均特征重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存结果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 输出平均训练和测试集准确率
    train_acc_mean = np.mean(model_accuracies[model_name]["train"])
    test_acc_mean = np.mean(model_accuracies[model_name]["test"])
    print(f"{model_name} 平均训练准确率：{train_acc_mean:.4f}")
    print(f"{model_name} 平均测试准确率：{test_acc_mean:.4f}")

# **3. 整合各模型的特征重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填补缺失值为 0（因为有些特征可能只出现在某些模型中）
combined_importance = combined_importance.fillna(0)

# 计算每个特征的综合重要性（加权平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序综合特征重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(10)

# 绘制综合特征重要性图表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 10 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 输出综合特征重要性表格
print("\n综合特征重要性（前 10 个）：")
print(combined_importance)

import shap
# **4. SHAP 分析，分别显示每个模型的因子增减影响**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每个模型的前 10 个特征
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 10 个特征重新训练模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 绘制 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.savefig(f"{model_name}_summary_plot.png")  # 保存图表
    plt.show()

"""##刪除FQC的資料"""

# Step 7: 刪除不必要的欄位
columns_to_drop = ['CheckBasID_QA', 'CheckSeq_QA', 'QABasID_QA', 'Thick_QA', 'mLotWidth_QA', 'EffectWidth_QA', 'TemperatureBefore_QA', 'PeelForceAvg_QA', 'RupturePeelForce_QA','VMThicknessBeforeLeft_QA', 'VMThicknessBeforeMiddle_QA', 'VMThicknessBeforeRight_QA', 'VMThicknessAfterLeft_QA', 'VMThicknessAfterMiddle_QA', 'VMThicknessAfterRight_QA', 'UFBSectionScoreFront_QA', 'UFBSectionScoreEnd_QA', 'UpForce_QA', 'PutMeters_QA', 'AMeters_QA', 'AFaultMeters_QA', 'A1Meters_QA', 'GiveMeters_QA', 'BFaultMeters_QA', 'CFaultMeters_QA', 'MRBMeters_QA', 'OverMeters_QA', 'ProductCode_QA', 'AOD_QA', 'Returned_QA', 'QAStat_QA', 'INCheck_QA', 'QtyReceived_QA', 'ArriveOnTimeStat_QA', 'ProductCodeSNo_QA', 'PoLine_QA', 'QAChecked_QA', 'QAResultSuccessFull_QA', 'KilogramToDrum_QA', 'QtyReceivedOfUnit_QA', 'QAResultErrorOfUnit_QA','ColorDataFrontL_QA', 'ColorDataEndL_QA',]
data_cleaned = data.drop(columns=[col for col in columns_to_drop if col in data.columns])

columns_to_drop=['ADDryBasis_QA']
data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_drop if col in data_cleaned.columns])

# 將欄位轉換為列表，方便完整顯示
columns_list = list(data_cleaned.columns)

# 打印欄位數和完整欄位名
print(f"總欄位數：{len(columns_list)}")
print(columns_list)

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from catboost import CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# 假设 data 已经标准化，并且处理了缺失值
X = data_cleaned.drop(columns=['Judgment_QA'])  # 独立变量
y = data_cleaned['Judgment_QA']  # 目标变量

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "CatBoost": CatBoostClassifier(verbose=0)
}

# 初始化交叉验证器
kfold = StratifiedKFold(n_splits=5, shuffle=True)

# **1. 独立挑选每个模型的前 10 特征**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑选 {model_name} 的前 10 个重要特征")
    feature_importances = []

    # 使用交叉验证计算特征重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特征重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 创建特征重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 选择前 10 个重要特征
    top_features = importance_df.head(10)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 10 特征：\n{top_features}")

# **2. 使用前 10 特征进行交叉验证训练**
final_feature_importance_results = {}
model_accuracies = {model_name: {"train": [], "test": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 10 特征进行重新训练和验证")
    top_features = selected_features_per_model[model_name]

    # 仅使用前 10 个特征
    X_selected = X[top_features]

    # 记录特征重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 记录训练集和测试集准确率
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        test_accuracy = accuracy_score(y_val_fold, y_val_pred)

        model_accuracies[model_name]["train"].append(train_accuracy)
        model_accuracies[model_name]["test"].append(test_accuracy)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 计算平均特征重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存结果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 输出平均训练和测试集准确率
    train_acc_mean = np.mean(model_accuracies[model_name]["train"])
    test_acc_mean = np.mean(model_accuracies[model_name]["test"])
    print(f"{model_name} 平均训练准确率：{train_acc_mean:.4f}")
    print(f"{model_name} 平均测试准确率：{test_acc_mean:.4f}")

# **3. 整合各模型的特征重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填补缺失值为 0（因为有些特征可能只出现在某些模型中）
combined_importance = combined_importance.fillna(0)

# 计算每个特征的综合重要性（加权平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序综合特征重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(10)

# 绘制综合特征重要性图表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 10 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 输出综合特征重要性表格
print("\n综合特征重要性（前 10 个）：")
print(combined_importance)

# **4. SHAP 分析，分别显示每个模型的因子增减影响**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每个模型的前 10 个特征
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 10 个特征重新训练模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 绘制 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.savefig(f"{model_name}_summary_plot.png")  # 保存图表
    plt.show()

"""##刪除IPQC"""

# 將欄位轉換為列表，方便完整顯示
columns_list = list(data_cleaned.columns)

# 打印欄位數和完整欄位名
print(f"總欄位數：{len(columns_list)}")
print(columns_list)

# Step 7: 刪除不必要的欄位
columns_to_drop = ['BasInspectionSeq_IPQC', 'Shift_IPQC', 'DtlInspectionSeq_IPQC', 'JudgeResult_IPQC', 'AppearanceDescription', 'AppearanceCheck', 'BSectionScoreEnd_IPQC', 'PeelForce_IPQC', 'RupturePeelForce_IPQC', 'DryBasis_IPQC','Diff_FinishDate_VDateTime_AD']
data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_drop if col in data_cleaned.columns])

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from catboost import CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

# 假设 data 已经标准化，并且处理了缺失值
X = data_cleaned.drop(columns=['Judgment_QA'])  # 独立变量
y = data_cleaned['Judgment_QA']  # 目标变量

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "CatBoost": CatBoostClassifier(verbose=0)
}

# 初始化交叉验证器
kfold = StratifiedKFold(n_splits=5, shuffle=True)

# **1. 独立挑选每个模型的前 10 特征**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑选 {model_name} 的前 10 个重要特征")
    feature_importances = []

    # 使用交叉验证计算特征重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特征重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 创建特征重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 选择前 10 个重要特征
    top_features = importance_df.head(10)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 10 特征：\n{top_features}")

# **2. 使用前 10 特征进行交叉验证训练**
final_feature_importance_results = {}
model_accuracies = {model_name: {"train": [], "test": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 10 特征进行重新训练和验证")
    top_features = selected_features_per_model[model_name]

    # 仅使用前 10 个特征
    X_selected = X[top_features]

    # 记录特征重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 训练模型
        model.fit(X_train_fold, y_train_fold)

        # 记录训练集和测试集准确率
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        test_accuracy = accuracy_score(y_val_fold, y_val_pred)

        model_accuracies[model_name]["train"].append(train_accuracy)
        model_accuracies[model_name]["test"].append(test_accuracy)

        # 获取特征重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 计算平均特征重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存结果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 输出平均训练和测试集准确率
    train_acc_mean = np.mean(model_accuracies[model_name]["train"])
    test_acc_mean = np.mean(model_accuracies[model_name]["test"])
    print(f"{model_name} 平均训练准确率：{train_acc_mean:.4f}")
    print(f"{model_name} 平均测试准确率：{test_acc_mean:.4f}")

# **3. 整合各模型的特征重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填补缺失值为 0（因为有些特征可能只出现在某些模型中）
combined_importance = combined_importance.fillna(0)

# 计算每个特征的综合重要性（加权平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序综合特征重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(10)

# 绘制综合特征重要性图表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 10 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 输出综合特征重要性表格
print("\n综合特征重要性（前 10 个）：")
print(combined_importance)

# **4. SHAP 分析，分别显示每个模型的因子增减影响**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每个模型的前 10 个特征
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 10 个特征重新训练模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 绘制 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.savefig(f"{model_name}_summary_plot.png")  # 保存图表
    plt.show()

"""##預測模型"""

data_cleaned

'''
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, accuracy_score
import matplotlib.pyplot as plt
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier

# 假设 data_cleaned 已经加载并清洗
# 定义重要特征和目标变量
important_features = [
    "StyPercentage_comps_AD", "StyPercentage_comps_RI", "FStatus_洗車_comps_Top",
    "Diff_FinishDate_VDateTime_AD", "B85_RI", "Diff_VDateTime_AL_AD",
    "NaturalGas_RI", "A84_AD", "B24_RI", "RSS_015_Field1_AD"
]
X = data_cleaned[important_features]
y = data_cleaned["Judgment_QA"]

# 分割数据并标准化
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 使用 SMOTE 上采样
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# 定义模型
models = {
    "LightGBM": lgb.LGBMClassifier(class_weight="balanced"),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric="logloss",
                                  scale_pos_weight=len(y_train_resampled[y_train_resampled == 0]) /
                                  len(y_train_resampled[y_train_resampled == 1])),
    "CatBoost": CatBoostClassifier(class_weights=[1, len(y_train_resampled[y_train_resampled == 0]) /
                                                   len(y_train_resampled[y_train_resampled == 1])], verbose=0)
}
weights = {"LightGBM": 0.3, "XGBoost": 0.3, "CatBoost": 0.4}

# 模型训练与预测
model_predictions = {}
for model_name, model in models.items():
    print(f"训练 {model_name} 模型...")
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict_proba(X_test_scaled)[:, 1]
    model_predictions[model_name] = y_pred

# 集成模型预测
ensemble_prediction = sum(model_predictions[model_name] * weights[model_name] for model_name in models)
threshold = 0.5
ensemble_result = (ensemble_prediction >= threshold).astype(int)

# 绘制混淆矩阵
cm = confusion_matrix(y_test, ensemble_result)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["NG", "OK"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Ensemble Model")
plt.show()

# 绘制 ROC 曲线
fpr, tpr, _ = roc_curve(y_test, ensemble_prediction)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Ensemble Model")
plt.legend(loc="lower right")
plt.grid()
plt.show()

# 绘制每个模型的预测概率分布
plt.figure(figsize=(8, 6))
for model_name, y_pred in model_predictions.items():
    plt.hist(y_pred, bins=20, alpha=0.5, label=model_name, density=True)
plt.xlabel("Predicted Probability")
plt.ylabel("Density")
plt.title("Prediction Probability Distribution Across Models")
plt.legend()
plt.grid()
plt.show()

import pandas as pd

# 将 scaler.mean_ 和 scaler.scale_ 转换为 Pandas Series
mean_series = pd.Series(scaler.mean_, index=numeric_columns)
scale_series = pd.Series(scaler.scale_, index=numeric_columns)

# 指定需要查找的列名列表
specific_columns = [
    "StyPercentage_comps_AD", "StyPercentage_comps_RI", "FStatus_洗車_comps_Top",
    "B85_RI", "Diff_VDateTime_AL_AD","A84_AD", "B24_RI", "RSS_015_Field1_AD",
    "A84_RI","StyPercentage_comps_Top"
]

# 筛选对应的 mean 和 scale
specific_means = mean_series[specific_columns]
specific_scales = scale_series[specific_columns]

print("特定欄位的均值 (means):")
print(specific_means)

print("\n特定欄位的標準差 (scales):")
print(specific_scales)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, accuracy_score
import matplotlib.pyplot as plt
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier

# 假设 data_cleaned 已经加载并清洗，且已经标准化
# 定义重要特征和目标变量
important_features = [
    "StyPercentage_comps_AD", "StyPercentage_comps_RI", "FStatus_洗車_comps_Top",
    "B85_RI", "Diff_VDateTime_AL_AD","A84_AD", "B24_RI", "RSS_015_Field1_AD",
    "A84_RI","StyPercentage_comps_Top"
]
X = data_cleaned[important_features]
y = data_cleaned["Judgment_QA"]

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义模型
models = {
    "LightGBM": lgb.LGBMClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
    "CatBoost": CatBoostClassifier(verbose=0)
}
weights = {"LightGBM": 0.34, "XGBoost": 0.33, "CatBoost": 0.33}

# 模型训练与预测
model_predictions = {}
for model_name, model in models.items():
    print(f"训练 {model_name} 模型...")
    model.fit(X_train, y_train)  # 不再标准化数据
    y_pred = model.predict_proba(X_test)[:, 1]  # 使用原始测试数据
    model_predictions[model_name] = y_pred

# 集成模型预测
ensemble_prediction = sum(model_predictions[model_name] * weights[model_name] for model_name in models)
threshold = 0.5
ensemble_result = (ensemble_prediction >= threshold).astype(int)

# 绘制混淆矩阵
cm = confusion_matrix(y_test, ensemble_result)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["NG", "OK"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Ensemble Model")
plt.show()

X_train

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [39],
    "Diff_VDateTime_AL_AD": [75],
    "A84_AD": [3.57],
    "B24_RI": [4.7],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.39],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)

# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
weights = {"LightGBM": 0.3, "XGBoost": 0.3, "CatBoost": 0.4}  # 模型权重
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [38],
    "Diff_VDateTime_AL_AD": [77],
    "A84_AD": [3.57],
    "B24_RI": [4.71],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.39],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)



# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

scaler.scale_

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [54],
    "Diff_VDateTime_AL_AD": [76],
    "A84_AD": [3.57],
    "B24_RI": [4.7],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.39],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)


# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [32],
    "Diff_VDateTime_AL_AD": [77],
    "A84_AD": [3.54],
    "B24_RI": [5.05],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.36],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)


# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [32],
    "Diff_VDateTime_AL_AD": [77],
    "A84_AD": [3.54],
    "B24_RI": [4.88],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.35],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)


# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

import pandas as pd

# 特定欄位的均值和標準差
means = {
    "StyPercentage_comps_AD": 0.000077,
    "StyPercentage_comps_RI": 0.000568,
    "FStatus_洗車_comps_Top": 0.041814,
    "B85_RI": 33.081587,
    "Diff_VDateTime_AL_AD": 49.160163,
    "A84_AD": 3.509349,
    "B24_RI": 5.219410,
    "RSS_015_Field1_AD": 0.791201,
    "A84_RI": 4.311425,
    "StyPercentage_comps_Top": 0.000053
}

scales = {
    "StyPercentage_comps_AD": 0.000203,
    "StyPercentage_comps_RI": 0.000975,
    "FStatus_洗車_comps_Top": 0.200165,
    "B85_RI": 68.079735,
    "Diff_VDateTime_AL_AD": 20.704395,
    "A84_AD": 0.265833,
    "B24_RI": 0.439644,
    "RSS_015_Field1_AD": 0.000234,
    "A84_RI": 0.343352,
    "StyPercentage_comps_Top": 0.000121
}

# 模擬假數據
fake_data = pd.DataFrame({
    "StyPercentage_comps_AD": [0],
    "StyPercentage_comps_RI": [0.000149],
    "FStatus_洗車_comps_Top": [0],
    "B85_RI": [35],
    "Diff_VDateTime_AL_AD": [79],
    "A84_AD": [3.53],
    "B24_RI": [4.59],
    "RSS_015_Field1_AD": [0.792],
    "A84_RI": [4.38],
    "StyPercentage_comps_Top": [0]
})

# 標準化假數據
fake_data_scaled = fake_data.copy()
for column in means.keys():
    fake_data_scaled[column] = (fake_data[column] - means[column]) / scales[column]

print("\n標準化後的假數據：")
print(fake_data_scaled)


# 使用各模型预测假数据并显示概率
fake_predictions = {}
print("\n假数据的预测概率：")
for model_name, model in models.items():
    prob = model.predict_proba(fake_data_scaled)[:, 1][0]  # 获取正类（OK）的概率
    fake_predictions[model_name] = prob
    print(f"{model_name} 预测的 OK 概率：{prob:.4f}")

# 集成模型预测
fake_ensemble_prediction = sum(
    fake_predictions[model_name] * weights[model_name] for model_name in models
)

# 计算 `NG` 的概率（1 - OK 概率）
fake_ensemble_ng_prob = 1 - fake_ensemble_prediction

# 显示集成模型的最终概率
print(f"\n集成模型预测的 OK 概率：{fake_ensemble_prediction:.4f}")
print(f"集成模型预测的 NG 概率：{fake_ensemble_ng_prob:.4f}")

# 转换为二分类结果
threshold = 0.5  # 阈值
fake_result = int(fake_ensemble_prediction >= threshold)
print(f"假数据预测结果 (Judgment_QA): {'OK' if fake_result == 1 else 'NG'}")

"""##影響AD乾量的因子"""

data_cleaned['ADDryBasis_QA']=data['ADDryBasis_QA']

columns_to_drop = ['Judgment_QA']
data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_drop if col in data_cleaned.columns])

# 將欄位轉換為列表，方便完整顯示
columns_list = list(data_cleaned.columns)

# 打印欄位數和完整欄位名
print(f"總欄位數：{len(columns_list)}")
print(columns_list)

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb

# 假設 data_cleaned 已經標準化，並且處理了缺失值
X = data_cleaned.drop(columns=['ADDryBasis_QA'])  # 自變數
y = data_cleaned['ADDryBasis_QA']  # 目標變數

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMRegressor(),
    "XGBoost": xgb.XGBRegressor(objective="reg:squarederror", eval_metric="rmse"),
    "CatBoost": CatBoostRegressor(verbose=0)
}

# 初始化交叉驗證器
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# **1. 為每個模型獨立挑選前 15 特徵**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑選 {model_name} 的前 15 個重要特徵")
    feature_importances = []

    # 使用交叉驗證計算特徵重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特徵重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 創建特徵重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 選擇前 15 個重要特徵
    top_features = importance_df.head(15)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 15 特徵：\n{top_features}")

# **2. 使用前 15 特徵進行交叉驗證訓練**
final_feature_importance_results = {}
model_metrics = {model_name: {"train": [], "test": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 15 特徵進行重新訓練和驗證")
    top_features = selected_features_per_model[model_name]

    # 僅使用前 15 個特徵
    X_selected = X[top_features]

    # 記錄特徵重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 預測
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        # 計算 R² 指標
        train_r2 = r2_score(y_train_fold, y_train_pred)
        test_r2 = r2_score(y_val_fold, y_val_pred)

        model_metrics[model_name]["train"].append(train_r2)
        model_metrics[model_name]["test"].append(test_r2)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 計算平均特徵重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存結果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 輸出平均訓練和測試集 R²
    train_r2_mean = np.mean(model_metrics[model_name]["train"])
    test_r2_mean = np.mean(model_metrics[model_name]["test"])
    print(f"{model_name} 平均訓練 R²：{train_r2_mean:.4f}")
    print(f"{model_name} 平均測試 R²：{test_r2_mean:.4f}")

# **3. 整合各模型的特徵重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填補缺失值為 0（因為有些特徵可能只出現在某些模型中）
combined_importance = combined_importance.fillna(0)

# 計算每個特徵的綜合重要性（加權平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序綜合特徵重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(15)

# 繪製綜合特徵重要性圖表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 15 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 輸出綜合特徵重要性表格
print("\n綜合特徵重要性（前 15 個）：")
print(combined_importance)

import shap

# **4. SHAP 分析，分別顯示每個模型的 Summary Plot**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每個模型的前 15 個特徵
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 15 個特徵重新訓練模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 繪製 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.show()

"""##需要保留RI收捲米數的資料!!!

##剝離力
"""

data_cleaned['PeelForceAvg_QA']=data['PeelForceAvg_QA']

columns_to_drop = ['ADDryBasis_QA']
data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_drop if col in data_cleaned.columns])

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb

# 假設 data_cleaned 已經標準化，並且處理了缺失值
X = data_cleaned.drop(columns=['PeelForceAvg_QA'])  # 自變數
y = data_cleaned['PeelForceAvg_QA']  # 目標變數

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMRegressor(),
    "XGBoost": xgb.XGBRegressor(objective="reg:squarederror", eval_metric="rmse"),
    "CatBoost": CatBoostRegressor(verbose=0)
}

# 初始化交叉驗證器
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# **1. 為每個模型獨立挑選前 15 特徵**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑選 {model_name} 的前 15 個重要特徵")
    feature_importances = []

    # 使用交叉驗證計算特徵重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特徵重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 創建特徵重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 選擇前 15 個重要特徵
    top_features = importance_df.head(15)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 15 特徵：\n{top_features}")

# **2. 使用前 15 特徵進行交叉驗證訓練**
final_feature_importance_results = {}
model_metrics = {model_name: {"train_r2": [], "test_r2": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 15 特徵進行重新訓練和驗證")
    top_features = selected_features_per_model[model_name]

    # 僅使用前 15 個特徵
    X_selected = X[top_features]

    # 記錄特徵重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 預測
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        # 計算 R² 指標
        train_r2 = r2_score(y_train_fold, y_train_pred)
        test_r2 = r2_score(y_val_fold, y_val_pred)

        model_metrics[model_name]["train_r2"].append(train_r2)
        model_metrics[model_name]["test_r2"].append(test_r2)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 計算平均特徵重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存結果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 輸出平均訓練和測試集 R²
    train_r2_mean = np.mean(model_metrics[model_name]["train_r2"])
    test_r2_mean = np.mean(model_metrics[model_name]["test_r2"])
    print(f"{model_name} 平均訓練 R²：{train_r2_mean:.4f}")
    print(f"{model_name} 平均測試 R²：{test_r2_mean:.4f}")

# **3. 整合各模型的特徵重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填補缺失值為 0（因為有些特徵可能只出現在某些模型中）
combined_importance = combined_importance.fillna(0)

# 計算每個特徵的綜合重要性（加權平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序綜合特徵重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(15)

# 繪製綜合特徵重要性圖表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 15 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 輸出綜合特徵重要性表格
print("\n綜合特徵重要性（前 15 個）：")
print(combined_importance)

import shap

# **4. SHAP 分析，分別顯示每個模型的 Summary Plot**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每個模型的前 15 個特徵
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 15 個特徵重新訓練模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 繪製 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.show()

"""##破膜力"""

data_cleaned['RupturePeelForce_QA']=data['RupturePeelForce_QA']

columns_to_drop = ['PeelForceAvg_QA']
data_cleaned = data_cleaned.drop(columns=[col for col in columns_to_drop if col in data_cleaned.columns])

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb

# 假設 data_cleaned 已經標準化，並且處理了缺失值
X = data_cleaned.drop(columns=['RupturePeelForce_QA'])  # 自變數
y = data_cleaned['RupturePeelForce_QA']  # 目標變數

# 初始化模型列表
models = {
    "LightGBM": lgb.LGBMRegressor(),
    "XGBoost": xgb.XGBRegressor(objective="reg:squarederror", eval_metric="rmse"),
    "CatBoost": CatBoostRegressor(verbose=0)
}

# 初始化交叉驗證器
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# **1. 為每個模型獨立挑選前 15 特徵**
selected_features_per_model = {}
for model_name, model in models.items():
    print(f"\n正在挑選 {model_name} 的前 15 個重要特徵")
    feature_importances = []

    # 使用交叉驗證計算特徵重要性
    for train_idx, val_idx in kfold.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        feature_importances.append(importance)

    # 平均特徵重要性
    avg_importance = np.mean(feature_importances, axis=0)

    # 創建特徵重要性 DataFrame
    importance_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": avg_importance
    }).sort_values(by="Importance", ascending=False)

    # 選擇前 15 個重要特徵
    top_features = importance_df.head(15)["Feature"].tolist()
    selected_features_per_model[model_name] = top_features
    print(f"{model_name} 的前 15 特徵：\n{top_features}")

# **2. 使用前 15 特徵進行交叉驗證訓練**
final_feature_importance_results = {}
model_metrics = {model_name: {"train_r2": [], "test_r2": []} for model_name in models.keys()}

for model_name, model in models.items():
    print(f"\n使用 {model_name} 的前 15 特徵進行重新訓練和驗證")
    top_features = selected_features_per_model[model_name]

    # 僅使用前 15 個特徵
    X_selected = X[top_features]

    # 記錄特徵重要性
    model_importances = []

    for train_idx, val_idx in kfold.split(X_selected, y):
        X_train_fold, X_val_fold = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

        # 訓練模型
        model.fit(X_train_fold, y_train_fold)

        # 預測
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        # 計算 R² 指標
        train_r2 = r2_score(y_train_fold, y_train_pred)
        test_r2 = r2_score(y_val_fold, y_val_pred)

        model_metrics[model_name]["train_r2"].append(train_r2)
        model_metrics[model_name]["test_r2"].append(test_r2)

        # 獲取特徵重要性
        if model_name == "CatBoost":
            importance = model.get_feature_importance()
        else:
            importance = model.feature_importances_
        model_importances.append(importance)

    # 計算平均特徵重要性
    avg_importance = np.mean(model_importances, axis=0)
    normalized_importance = avg_importance / np.sum(avg_importance)

    # 保存結果
    final_feature_importance_results[model_name] = pd.DataFrame({
        "Feature": top_features,
        "Importance": normalized_importance
    }).sort_values(by="Importance", ascending=False)

    # 輸出平均訓練和測試集 R²
    train_r2_mean = np.mean(model_metrics[model_name]["train_r2"])
    test_r2_mean = np.mean(model_metrics[model_name]["test_r2"])
    print(f"{model_name} 平均訓練 R²：{train_r2_mean:.4f}")
    print(f"{model_name} 平均測試 R²：{test_r2_mean:.4f}")

# **3. 整合各模型的特徵重要性**
combined_importance = pd.DataFrame(columns=["Feature"])
for model_name, importance_df in final_feature_importance_results.items():
    importance_df = importance_df.rename(columns={"Importance": f"Importance_{model_name}"})
    if combined_importance.empty:
        combined_importance = importance_df
    else:
        combined_importance = combined_importance.merge(
            importance_df, on="Feature", how="outer"
        )

# 填補缺失值為 0（因為有些特徵可能只出現在某些模型中）
combined_importance = combined_importance.fillna(0)

# 計算每個特徵的綜合重要性（加權平均或直接平均）
combined_importance["Overall_Importance"] = combined_importance[[col for col in combined_importance.columns if "Importance" in col]].mean(axis=1)

# 排序綜合特徵重要性
combined_importance = combined_importance.sort_values(by="Overall_Importance", ascending=False).head(15)

# 繪製綜合特徵重要性圖表
plt.figure(figsize=(10, 8))
plt.barh(combined_importance["Feature"], combined_importance["Overall_Importance"])
plt.gca().invert_yaxis()
plt.title("Overall Feature Importances (Top 15 Features)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# 輸出綜合特徵重要性表格
print("\n綜合特徵重要性（前 15 個）：")
print(combined_importance)

import shap

# **4. SHAP 分析，分別顯示每個模型的 Summary Plot**
for model_name, model in models.items():
    print(f"\n正在生成 {model_name} 的 SHAP Summary Plot")

    # 使用每個模型的前 15 個特徵
    top_features = selected_features_per_model[model_name]
    X_selected = X[top_features]

    # 使用前 15 個特徵重新訓練模型
    model.fit(X_selected, y)

    # 生成 SHAP 值
    explainer = shap.Explainer(model, X_selected)
    shap_values = explainer(X_selected)

    # 繪製 SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_selected, show=False)
    plt.title(f"SHAP Summary Plot for {model_name}")
    plt.tight_layout()
    plt.show()

